{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from module.prepare import *\n",
    "from itertools import product\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "from itertools import islice\n",
    "import time\n",
    "import configparser\n",
    "import json\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import sklearn\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.manifold import *\n",
    "import sklearn.tree as Tr \n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    @new_params whether to append new param\n",
    "    @default whether to use default param\n",
    "    @replace whether to replace old param\n",
    "    @tuning whethe to tuning with GridSearchCV\n",
    "    \n",
    "    @return [Ypred,Ytest,score_train,clf]\n",
    "'''\n",
    "def LGBTuning(Xtrain,Xtest,Ytrain,Ytest,new_params=None,default=False,replace=False,tuning=True):\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "                             silent=False,\n",
    "                             verbose=1,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=12,\n",
    "#                              class_weight\n",
    "                            )\n",
    "    \n",
    "    gridParams = {\n",
    "        # step 1\n",
    "#     'learning_rate': [0.01,0.05,0.1],\n",
    "#     'boosting_type':['gbdt','goss'],\n",
    "#     'n_estimators': [50,200,500],\n",
    "#     'num_iterations':[200,400,1000],\n",
    "        # step 1 fixed\n",
    "    'learning_rate': [0.1], ### 0.1\n",
    "    'boosting_type':['gbdt'], ### goss>gbdt\n",
    "    'n_estimators': [500],\n",
    "    'num_iterations':[2000], ### 2000\n",
    "#     'max_depth':[6], ### <6\n",
    "        # step 2\n",
    "#     'num_leaves': [200], ### <400<675\n",
    "#     'min_data_in_leaf':[18,20,22], ### 20 default\n",
    "#     'max_bin':[127,255,511],\n",
    "        # step 2 fixed\n",
    "#     'num_leaves': [800],\n",
    "    'max_bin':[256],\n",
    "        # step 3\n",
    "#     'max_depth':[7,8,9,10], ### missed\n",
    "    'colsample_bytree' : [0.9], ### 0.75\n",
    "    'subsample_freq':[1], ### 1\n",
    "        \n",
    "#     'subsample' : [0.7], ### 1\n",
    "#     'min_data_in_leaf':[26],\n",
    "#     'early_stopping_round':[2],\n",
    "#     'reg_alpha' : [1e-3], ### 0\n",
    "#     'reg_lambda' : [0,0.1,0.5], ### 0\n",
    "    }\n",
    "    \n",
    "    if replace is True:\n",
    "        gridParams = {}\n",
    "    if new_params is not None:\n",
    "        gridParams.update(new_params)\n",
    "    \n",
    "\n",
    "    if tuning:\n",
    "        grid = GridSearchCV(clf, gridParams,\n",
    "                        scoring='accuracy',\n",
    "                        verbose=3,\n",
    "                        cv=5,\n",
    "                        n_jobs=1)\n",
    "        print('default params\\n',clf.get_params())\n",
    "        grid.fit(Xtrain,Ytrain)\n",
    "        return grid\n",
    "    else:\n",
    "        if not default:\n",
    "            arg_str = ''\n",
    "            for k,v in gridParams.items():\n",
    "                if type(v[0])==str:\n",
    "                    arg_str += k+'='+\"'\"+v[0]+\"',\"\n",
    "                else:\n",
    "                    arg_str += k+'='+str(v[0])+\",\"\n",
    "            eval(\n",
    "                'clf.'+clf.set_params.__name__+\"(\"\n",
    "                    +arg_str.rstrip(',')+\n",
    "                    \")\"\n",
    "                )\n",
    "#         clf.class_weight = {1:sum(Ytrain==1),0:sum(Ytrain==0)}\n",
    "        print('params\\n',clf.get_params())\n",
    "        clf.fit(Xtrain,Ytrain)\n",
    "        Ypred = clf.predict(Xtest)\n",
    "        score_train = clf.score(Xtrain,Ytrain)\n",
    "        print('train score %f'%score_train)\n",
    "        return [Ypred,Ytest,score_train,clf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'data_NPInter10412',\n",
    "'reRPI2825',\n",
    "'RPI488',\n",
    "'RPI2241',\n",
    "'RPITER_RPI1807'\n",
    "'''\n",
    "\n",
    "hyper_params = GetConfigure()\n",
    "num_hyper_params = len(hyper_params)\n",
    "lassocv_param = {'threshold':0.05}\n",
    "\n",
    "cv = 5\n",
    "generalize_ratio = 1.0/cv\n",
    "test_ratio = 1.0/cv\n",
    "\n",
    "mi_use = False\n",
    "outside_grid = True\n",
    "tuning_mode = False\n",
    "\n",
    "if tuning_mode:\n",
    "    cv = 1\n",
    "\n",
    "cv_results = []\n",
    "# scores = []\n",
    "'''\n",
    "    topK =3000\n",
    "'''\n",
    "\n",
    "\n",
    "search_param = [{'learning_rate':[0.05,0.1,0.01,0.02],\n",
    "                'colsample_bytree':[0.7,0.8,0.9,1],\n",
    "                 'max_depth':[5,6,7,8,9],\n",
    "                 'num_iterations':[500,1000,2000],\n",
    "#                  'min_data_in_leaf':[20,25,30]\n",
    "                }]\n",
    "search_grid = list(ParameterGrid(search_param))\n",
    "\n",
    "conf_param = {}\n",
    "\n",
    "\n",
    "cfile = GetConfigureObject()\n",
    "commons = dict(cfile.items('common'))\n",
    "\n",
    "# db = redis.StrictRedis(host=commons[str.lower('REDIS_HOST')], port=6379, db=0, \n",
    "#                       password=commons[str.lower('REDIS_PWD')], decode_responses=True\n",
    "#                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def TuningParametersStage1():\n",
    "    res = []\n",
    "    for DATAID in [0,1,2,3,4]:\n",
    "        INFO('data id %d'%DATAID)\n",
    "        for RNA_K in range(3,7):\n",
    "            for PROTEIN_K in range(3,7):\n",
    "                for TOP_RATIO in np.linspace(0.93,0.99,5):\n",
    "                    [data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "                    [X,Y] = ToMatrix(data,'dense')\n",
    "                    [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "                    [X_train,X_test,Y_train,Y_test] = \\\n",
    "                                RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=0.96)\n",
    "                    r = LGBTuning(X_train,X_test,Y_train,Y_test,tuning=False,default=True)\n",
    "                    r = {'test_score':scoreFunction([r[0],r[1]]),\n",
    "                        'train_score':r[2],\n",
    "                        'RNA_K':RNA_K,\n",
    "                        'PROTEIN_K':PROTEIN_K,\n",
    "                        'TOP_RATIO':TOP_RATIO}\n",
    "                    res.append(r)\n",
    "                    WriteDictResult(DATAID,r,'3-14-result-of-k')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTuningStage1Result():\n",
    "    fname = './3-14-result-of-k'\n",
    "    with open(fname+'.txt','r+') as f:\n",
    "        text = f.read()\n",
    "        content = text.split('\\n')\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for line in content:\n",
    "        count += 1\n",
    "        if len(line):\n",
    "            if line[0] is '@':\n",
    "                if count>1:\n",
    "                    df = df.append([\n",
    "                        [\n",
    "                           r['DATAID'],\n",
    "                           r['RNA_K'],\n",
    "                           r['PROTEIN_K'],\n",
    "                           r['TOP_RATIO'],\n",
    "                           eval(r['test_score'])['acc'],\n",
    "                           eval(r['test_score'])['auc'],\n",
    "                           r['train_score']\n",
    "                        ]\n",
    "                                   ])\n",
    "                r = {}\n",
    "                r['DATAID'] = line[1]\n",
    "            else:\n",
    "                line = line.split('=\\t')\n",
    "                if line[0] is 'test_score':\n",
    "                    r['test_score'] = dict(eval(line[1]))\n",
    "                else:\n",
    "                    r[line[0]] = line[1]\n",
    "\n",
    "    df.columns=['DATAID','RNA_K','PROTEIN_K',\n",
    "               'TOP_RATIO',\n",
    "               'acc','auc','train_score']\n",
    "    return df\n",
    "df = getTuningStage1Result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fname_optimal_stage1 = './3-14-data-param-1.csv'\n",
    "fname_result = './3-18-cv-tune-1.csv'\n",
    "\n",
    "tune_grid = [\n",
    "    [{\n",
    "#         'learning_rate': [0.1], ### 0.1\n",
    "#         'boosting_type':['gbdt'], ### goss>gbdt\n",
    "#         'n_estimators': [500],\n",
    "#         'num_iterations':[1000,2000], ### 2000\n",
    "#         'num_leaves': [200], ### <400<675\n",
    "#         'max_bin':[256],\n",
    "#         'colsample_bytree' : [0.7,0.75,0.8,0.85], ### 0.75\n",
    "#         'subsample_freq':[1], ### 1\n",
    "#         'lambda_l1': [1e-3,0], \n",
    "        'boosting_type': ['gbdt'], \n",
    "        'colsample_bytree': [0.8], \n",
    "        'lambda_l1': [0], \n",
    "        'learning_rate': [0.1], \n",
    "        'max_bin': [256], \n",
    "        'n_estimators': [500], \n",
    "        'num_iterations': [2000], \n",
    "        'num_leaves': [200], \n",
    "        'subsample_freq': [1],\n",
    "    }],\n",
    "    [{\n",
    "#         'colsample_bytree': [1], \n",
    "#         'lambda_l1': [0], \n",
    "#         'learning_rate': [0.05], \n",
    "#         'min_data_in_leaf': [12,13,14,15], \n",
    "#         'num_iterations': [1000], \n",
    "#         'num_leaves': [80,100,120,150,200],\n",
    "        'colsample_bytree': [1], \n",
    "        'lambda_l1': [0], \n",
    "        'learning_rate': [0.05], \n",
    "        'min_data_in_leaf': [14], \n",
    "        'num_iterations': [1000], \n",
    "        'num_leaves': [100],\n",
    "    }],\n",
    "    [{\n",
    "#         'colsample_bytree': [0.7,0.8,0.85,0.9,0.95], \n",
    "#         'lambda_l1': [0], # =\n",
    "#         'learning_rate': [0.05,0.01,0.1], \n",
    "#         'min_data_in_leaf': [20], # =\n",
    "#         'num_iterations': [500,1000,2000],\n",
    "#         'num_leaves': [50],\n",
    "        'colsample_bytree': [0.9], \n",
    "        'lambda_l1': [0], \n",
    "        'learning_rate': [0.05], \n",
    "        'min_data_in_leaf': [18], \n",
    "        'num_iterations': [1000], \n",
    "        'num_leaves': [50],\n",
    "     }],\n",
    "    [{\n",
    "#         'colsample_bytree': [0.8,0.85,0.9,0.95],  \n",
    "#         'lambda_l1': [0.001], \n",
    "#         'learning_rate': [0.1,0.001,0.5],  \n",
    "#         'max_depth': [3,4,6,-1], \n",
    "#         'min_data_in_leaf': [10,18,30], \n",
    "#         'num_iterations': [500,1000,2000], \n",
    "#         'num_leaves': [10,40,60,80,100,120],\n",
    "        'colsample_bytree': [0.9], \n",
    "        'lambda_l1': [0.001], \n",
    "        'learning_rate': [0.5], \n",
    "        'max_depth': [-1], \n",
    "        'min_data_in_leaf': [10], \n",
    "        'num_iterations': [1000], \n",
    "        'num_leaves': [10],\n",
    "     }],\n",
    "    [{\n",
    "#         'colsample_bytree': [0.8,0.85,0.9,0.95],  \n",
    "#         'lambda_l1': [0], \n",
    "#         'learning_rate': [0.01,0.001,0.5], \n",
    "#         'max_depth': [3,4,6,-1],  \n",
    "#         'min_data_in_leaf': [10,18,30], \n",
    "#         'num_iterations': [500,1000,2000], \n",
    "#         'num_leaves': [10,40,60,80,100,120],\n",
    "        'colsample_bytree': [0.95], \n",
    "        'lambda_l1': [0], \n",
    "        'learning_rate': [0.5], \n",
    "        'max_depth': [-1], \n",
    "        'min_data_in_leaf': [10], \n",
    "        'num_iterations': [500], \n",
    "        'num_leaves': [40],\n",
    "     }]\n",
    "]\n",
    "\n",
    "tune_grid = list(map(lambda x:list(ParameterGrid(x)),tune_grid))\n",
    "\n",
    "tuning_cv2 = 10\n",
    "tuning_generalize_ratio2 = 1.0/tuning_cv2\n",
    "df_optimal = pd.read_csv(fname_optimal_stage1)\n",
    "\n",
    "df_columns = ['dataid','cv','training_score','tune_param','acc','auc',\n",
    "              'fpr','tpr','mcc','tnr','ppv','f_score','ap','brier','recall']\n",
    "df_result2 = pd.DataFrame([],columns=df_columns)\n",
    "\n",
    "for _dataid in [0,1,2,3,4]:\n",
    "    INFO('dataid %d'%_dataid)\n",
    "    ### get conf of dataid\n",
    "    df_current = df_optimal.loc[df_optimal['DATAID']==_dataid]\n",
    "    rna_k = df_current['RNA_K'].to_numpy()[0]\n",
    "    protein_k = df_current['PROTEIN_K'].to_numpy()[0]\n",
    "    top_ratio = df_current['TOP_RATIO'].to_numpy()[0]\n",
    "    ### read data\n",
    "    [data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "    [X,Y] = ToMatrix(data,'dense')\n",
    "    ### split dataset\n",
    "    [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,tuning_generalize_ratio2)\n",
    "    ### dimensionality reduction\n",
    "    [X_train,X_test,Y_train,Y_test] = \\\n",
    "                RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "    for _cv in range(tuning_cv2):\n",
    "        INFO('tuning cv %d'%_cv)\n",
    "        for sp in tune_grid[_dataid]:\n",
    "            sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "            tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp,tuning=False)\n",
    "            tune_score = scoreFunction(tune_results)\n",
    "            r = pd.Series({\n",
    "                            'dataid':_dataid,\n",
    "                            'cv':_cv,\n",
    "                            'training_score':tune_results[2],\n",
    "                            'tune_param':str(sp),\n",
    "                            'acc':tune_score['acc'],\n",
    "                            'auc':tune_score['auc'],\n",
    "                            'fpr':tune_score['fpr'],\n",
    "                            'tpr':tune_score['tpr'],\n",
    "                            'mcc':tune_score['mcc'],\n",
    "                            'tnr':tune_score['tnr'],\n",
    "                            'ppv':tune_score['ppv'],\n",
    "                            'f_score':tune_score['f_score'],\n",
    "                            'ap':tune_score['ap'],\n",
    "                            'brier':tune_score['brier'],\n",
    "                            'recall':tune_score['recall'],\n",
    "            })\n",
    "            df_result2 = df_result2.append(r,ignore_index=True)\n",
    "            \n",
    "df_result2.to_csv(fname_result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./3-18-cv-tune-1.csv')\n",
    "keys = ['acc','auc','mcc','tnr','ppv','f_score','ap','brier','recall']\n",
    "key_by = 'dataid'\n",
    "\n",
    "indexes = []\n",
    "indexes.extend(keys)\n",
    "indexes.append(key_by)\n",
    "\n",
    "key_agg = dict(map(lambda x:(x,np.mean),keys))\n",
    "\n",
    "# print(df)\n",
    "\n",
    "df_temp = df.groupby(by=[key_by]) \\\n",
    "                .aggregate(key_agg).reset_index()\n",
    "\n",
    "df_result = df.join(df_temp.set_index(indexes),on=indexes,how='right')\n",
    "\n",
    "df_result\n",
    "# df_result.to_csv('./3-17-param-tune-4-result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv result processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./3-18-cv-tune-1.csv')\n",
    "key = 'auc'\n",
    "key_by = 'dataid'\n",
    "df_temp = df.groupby(by=[key_by]) \\\n",
    "                .aggregate({key:np.mean}).reset_index()\n",
    "\n",
    "df_result = df.join(df_temp.set_index([key_by,key]),on=[key_by,key],how='inner')\n",
    "\n",
    "df_result\n",
    "df_result.to_csv('./3-18-cv-tune-result-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['DATAID']=='0')\n",
    "      & (df['RNA_K']=='3')\n",
    "      & ( (df['PROTEIN_K']=='3') | (df['PROTEIN_K']=='4') )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for batch in range(cv):\n",
    "    INFO('cross validation batch %d'%batch)\n",
    "    if mi_use==True:\n",
    "        arr = ToMatrix(data,'sparse')\n",
    "        [X_train,X_test,Y_train,Y_test] = MutualInformationFeatureSelection2(arr,data,generalize_ratio)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    else:\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "        [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "#         X_train,X_test,Y_train,Y_test = IsomapDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = LassoCVFeatureSelection(X_train,X_test,Y_train,Y_test,lassocv_param)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = \\\n",
    "#             PCADimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    if tuning_mode:\n",
    "        [Xtrain,Ytrain] = merge_train_test(X_train,X_test,Y_train,Y_test)\n",
    "        grid = LGBTuning(Xtrain,[],Ytrain,[],True)\n",
    "        cv_results.append(grid)\n",
    "        break\n",
    "    else:\n",
    "        if outside_grid is True:\n",
    "            for sp in search_grid:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp,tuning=False)\n",
    "                r = {'batch':batch,'res':tune_results,'sp':sp,'score':scoreFunction(tune_results)}\n",
    "                cv_results.append(r)\n",
    "            break\n",
    "        else:\n",
    "            tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,tuning=False,default=False)\n",
    "            r = {'batch':batch,'res':tune_results,'score':scoreFunction(tune_results)}\n",
    "            cv_results.append(r)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### compute scores\n",
    "scores = [scoreFunction(r['res'])for r in cv_results]\n",
    "auc = np.mean([x['auc'] for x in scores])\n",
    "acc = np.mean([x['acc'] for x in scores])\n",
    "scores,auc,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### save model\n",
    "if outside_grid is True:\n",
    "    search_param = set( list(map(lambda x:x['sp'],cv_results)) )\n",
    "    scores = [{sp:[] for sp in search_param}]\n",
    "    for sp in search_param:\n",
    "        scores[sp] = np.mean(list( map(lambda x:x['score'],filter(lambda x:x['sp']==sp,cv_results)) ))\n",
    "    best_sp = sorted(scores,key=lambda x:scores[x][0],reverse=True)[0]['sp']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATAID = 1\n",
    "PROTEIN_K = 3\n",
    "RNA_K = 6\n",
    "topRatio = 0.99\n",
    "[data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "[X,Y] = ToMatrix(data,'dense')\n",
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=0.96)\n",
    "r1 = LGBTuning(X_train,X_test,Y_train,Y_test,tuning=False,default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('cb', cb.CatBoostClassifier(verbose=0)),\n",
    "    ('lgb', lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "                             silent=False,\n",
    "                             verbose=1,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=12,\n",
    "                            )),\n",
    "    ('xgb',xgb.XGBClassifier()),\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegressionCV()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train,Y_train).score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreFunction([r1[0],r1[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "# clf_xgb = xgb.XGBClassifier()\n",
    "# clf_xgb.fit(X_train,Y_train).score(X_test,Y_test)\n",
    "clf_cb = cb.CatBoostClassifier()\n",
    "clf_cb.fit(X_train,Y_train).score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [score for grid in cv_results for score in grid.cv_results_['mean_test_score']]\n",
    "sns.distplot(scores,rug=True,bins=20)\n",
    "plt.show()\n",
    "\n",
    "# param_rank = np.array([grid.cv_results_['mean_test_score'],grid.cv_results_['params']]).T\n",
    "# a = sorted(param_rank,key=lambda x:x[0],reverse=True)\n",
    "# a = np.array(list(a))\n",
    "\n",
    "# a\n",
    "max( cv_results,key=lambda x:np.mean(x.cv_results_['mean_test_score']) ).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list( sorted(grid.best_estimator_.feature_importances_,reverse=True) )\n",
    "sum(feature_importance[:2000])/sum(feature_importance)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(grid.best_estimator_.feature_importances_,bins=100)\n",
    "plt.xlabel('feature importance')\n",
    "plt.ylabel('ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
