{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from module.prepare import *\n",
    "from itertools import product\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "from itertools import islice\n",
    "import time\n",
    "import configparser\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import sklearn\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.manifold import *\n",
    "import sklearn.tree as Tr \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import optuna.integration.lightgbm\n",
    "\n",
    "from module.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentTime():\n",
    "    return datetime.datetime.strftime(datetime.datetime.fromtimestamp(time.time()),format='%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "\n",
    "def LGBOptuna(trial):\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    " \n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    " \n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(test_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, pred_labels)\n",
    "    return accuracy\n",
    " \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "    return\n",
    "\n",
    "def LGBTuning(Xtrain,Xtest,Ytrain,Ytest,new_params=None):\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "#                              silent=False,\n",
    "                             verbose=0,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=20,\n",
    "#                              class_weight\n",
    "                            )\n",
    "    \n",
    "    default_params = {\n",
    "    'learning_rate': [0.1], \n",
    "    'boosting_type':['gbdt'], \n",
    "    'n_estimators': [500],\n",
    "    'num_iterations':[1000],\n",
    "    'max_bin':[256]\n",
    "    }\n",
    "    \n",
    "    if new_params is not None:\n",
    "        default_params.update(new_params)\n",
    "    \n",
    "    arg_str = ''\n",
    "    for k,v in default_params.items():\n",
    "        if type(v[0])==str:\n",
    "            arg_str += k+'='+\"'\"+v[0]+\"',\"\n",
    "        else:\n",
    "            arg_str += k+'='+str(v[0])+\",\"\n",
    "    eval(\n",
    "        'clf.'+clf.set_params.__name__+\"(\"\n",
    "            +arg_str.rstrip(',')+\n",
    "            \")\"\n",
    "        )\n",
    "\n",
    "#     print('DEBUG:: tuning params\\n',clf.get_params())\n",
    "    clf.fit(Xtrain,Ytrain)\n",
    "    Ypred = clf.predict(Xtest)\n",
    "    score_train = clf.score(Xtrain,Ytrain)\n",
    "    print('train score %f'%score_train)\n",
    "    \n",
    "    return [Ypred,Ytest,score_train,clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "generalize_ratio = 1.0/cv\n",
    "test_ratio = 1.0/cv\n",
    "tuning_mode = False\n",
    "\n",
    "if tuning_mode:\n",
    "    cv = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def TuningParametersStage1(fname=getCurrentTime()+'-stage1.csv'):\n",
    "    res = []\n",
    "    for DATAID in [3]:\n",
    "        INFO('data id %d'%DATAID)\n",
    "        for RNA_K in range(3,7):\n",
    "            for PROTEIN_K in range(3,7):\n",
    "                for TOP_RATIO in np.linspace(0.93,0.99,5):\n",
    "                    start_time = time.time()\n",
    "                    [data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "                    [X,Y] = ToMatrix(data,'dense')\n",
    "                    \n",
    "                    for _cv in range(5): ### cv testing\n",
    "                        [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "                        [X_train,X_test,Y_train,Y_test] = \\\n",
    "                                    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=TOP_RATIO)\n",
    "                        r = LGBTuning(X_train,X_test,Y_train,Y_test)\n",
    "                        r = {\n",
    "                            'DATAID': DATAID,\n",
    "                            'test_score':scoreFunction(r[0],r[1]),\n",
    "                            'train_score':r[2],\n",
    "                            'RNA_K':RNA_K,\n",
    "                            'PROTEIN_K':PROTEIN_K,\n",
    "                            'TOP_RATIO':TOP_RATIO,\n",
    "                            'cv':_cv,\n",
    "                        }\n",
    "                        print('DEBUG:: result ',r)\n",
    "                        res.append(r)\n",
    "                    end_time = time.time()\n",
    "                    print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "    df = pd.DataFrame(data=res,columns=['DATAID','test_score','train_score','RNA_K','PROTEIN_K','TOP_RATIO'])\n",
    "    df.to_csv(os.path.join('./result',fname))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TuningParametersStage1('data-3-global-tune-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "'''\n",
    "['NPInter10412','reRPI2825','RPI488','RPI2241','RPI1807','LPI43250','EVLncRNAs']\n",
    "'''\n",
    "params_1 = {\n",
    "    0:[(5,5,0.99)],\n",
    "    1:[(3,4,0.96)], # stage1\n",
    "    2:[(4,6,0.96)],\n",
    "    3:[(3,3,0.975),(6,3,0.93),(4,3,0.975)],\n",
    "    4:[(4,3,0.96)],\n",
    "    5:[(4,6,0.945)]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_result2 = './result/'+getCurrentTime()+'-stage2-3.csv'\n",
    "\n",
    "tune_grid = [\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.1], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "    }],\n",
    "    [{\n",
    "        'learning_rate': [0.025,0.03,0.035,0.04], ### 0.1\n",
    "        'boosting_type':['gbdt'], ### goss>gbdt\n",
    "        'n_estimators': [500],\n",
    "        'num_iterations':[2000], ### 2000\n",
    "        'max_depth': [8,9,10], ### <400<675\n",
    "        'max_bin':[256],\n",
    "        'colsample_bytree' : [0.95,1], ### 0.75\n",
    "        'bagging_fraction':[0.9,0.95,1], ### 1\n",
    "        'bagging_freq':[1,2,3],\n",
    "        'lambda_l1': [0.075,0.1,0.125],\n",
    "    }],\n",
    "    [{\n",
    "        \"bagging_fraction\": [0.95], \n",
    "        \"bagging_freq\": [2], \n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"colsample_bytree\": [1], \n",
    "        \"lambda_l1\": [0.01], \n",
    "        \"learning_rate\": [0.2], \n",
    "        \"max_bin\": [256], \n",
    "        \"max_depth\": [7], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "        \n",
    "#         'learning_rate': [0.15,0.2,0.25], ### 0.1\n",
    "#         'boosting_type':['gbdt'], ### goss>gbdt\n",
    "#         'n_estimators': [500],\n",
    "#         'num_iterations':[1500,2000], ### 2000\n",
    "#         'max_depth': [5,7,9], ### <400<675\n",
    "#         'max_bin':[256],\n",
    "#         'colsample_bytree' : [0.8,0.9,1], ### 0.75\n",
    "#         'bagging_fraction':[0.9,0.95,1], ### 1\n",
    "#         'bagging_freq':[2,3,4],\n",
    "#         'lambda_l1': [0.005,0.01,0.015],\n",
    "     }],\n",
    "    [{\n",
    "#         \"boosting_type\": [\"gbdt\"], \n",
    "#         \"colsample_bytree\": [0.8], \n",
    "#         \"learning_rate\": [0.05], \n",
    "#         \"max_bin\": [256], \n",
    "#         \"n_estimators\": [500], \n",
    "#         \"num_iterations\": [1000],\n",
    "        \n",
    "        'learning_rate': [0.15,0.2,0.25], ### 0.1\n",
    "        'boosting_type':['gbdt'], ### goss>gbdt\n",
    "        'n_estimators': [500],\n",
    "        'num_iterations':[2000], ### 2000\n",
    "        'max_depth': [12,13,14,15], ### <400<675\n",
    "        'max_bin':[256],\n",
    "        'colsample_bytree' : [0.7,0.8,1], ### 0.75\n",
    "        'bagging_fraction':[0.82,0.9,1], ### 1\n",
    "#         'bagging_freq':[2,3,4],\n",
    "        'lambda_l1': [0,0.01,0.02],\n",
    "     }],\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.01], \n",
    "        \"max_bin\": [256], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "     }],\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.1], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [1000],\n",
    "     }]\n",
    "]\n",
    "\n",
    "tune_grid = list(map(lambda x:list(ParameterGrid(x)),tune_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning_cv2 = 5\n",
    "tuning_generalize_ratio2 = 1.0/tuning_cv2 if tuning_cv2!=1 else 0.2\n",
    "\n",
    "df_columns = ['dataid','protein_k','rna_k','top_ratio','training_score','tune_param','scores','cv']\n",
    "df_result2 = pd.DataFrame([],columns=df_columns)\n",
    "\n",
    "count = 0\n",
    "for _dataid in [3]:\n",
    "    INFO('dataid %d'%_dataid)\n",
    "    for global_params in params_1[_dataid]:\n",
    "        \n",
    "        ### get conf of dataid\n",
    "        protein_k = global_params[0]\n",
    "        rna_k = global_params[1]\n",
    "        top_ratio = global_params[2]\n",
    "        ### read data\n",
    "        [data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "\n",
    "        for _cv in range(tuning_cv2):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            INFO('tuning cv %d'%_cv)\n",
    "            \n",
    "            ### split dataset\n",
    "            [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,tuning_generalize_ratio2)\n",
    "            ### dimensionality reduction\n",
    "            [X_train,X_test,Y_train,Y_test] = \\\n",
    "                        RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "            \n",
    "            for sp in tune_grid[_dataid]:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp)\n",
    "                tune_score = scoreFunction(tune_results[0],tune_results[1])\n",
    "                r = pd.Series({\n",
    "                                'dataid':_dataid,\n",
    "                                'protein_k':protein_k,\n",
    "                                'rna_k':rna_k,\n",
    "                                'top_ratio':top_ratio,\n",
    "                                'training_score':tune_results[2],\n",
    "                                'tune_param':json.dumps(sp),\n",
    "                                'scores':json.dumps(tune_score),\n",
    "                                'cv':_cv\n",
    "                })\n",
    "                df_result2 = df_result2.append(r,ignore_index=True)\n",
    "                count += 1\n",
    "                if count%100==1:\n",
    "                    print(dict(r))\n",
    "    \n",
    "            end_time = time.time()\n",
    "            print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "df_result2.to_csv(fname_result2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data 3 3\n",
      "=============RETRIEVE TRIAN DATA=================\n",
      "# DEBUG: # DEBUG: **************new dl 3***************\n",
      "# DEBUG: READ SEQ FROM FILE\n",
      "# DEBUG: READ CLUSTER FROM FILE\n",
      "ERROR:: regex  \n",
      "ERROR:: regex  \n",
      "# DEBUG: READ PAIR FROM FILE\n",
      "# DEBUG: GENERATE NEGATIVE PAIR\n",
      "# DEBUG: negative pair number 2240\n",
      "INFO::count of negative pairs2240\n",
      "# DEBUG: PAIR UNION\n",
      "# DEBUG: EXTRACT FEATURES--PROTEIN\n",
      "# DEBUG: EXTRACT FEATURES--RNA\n",
      "# DEBUG: K-MER CALCULATION\n",
      "# DEBUG: FEATURE UNION\n",
      "# DEBUG: GARBAGE COLLECTION\n",
      "MATRIX TRANSFORMATION\n",
      "DEBUG:: total features count  2375\n",
      "data shape 4480 2375\n",
      "rf raw data fit score 0.999721\n",
      "INFO::dimension remained 2345 0.975000\n",
      "dimension remained 2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\optuna\\_experimental.py:111: ExperimentalWarning: RedisStorage is experimental (supported from v1.4.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "[I 2020-05-24 15:19:07,735] Finished trial#0 with value: 0.8091517857142857 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0011967148615539519, 'hidden_layer_sizes': 570.0, 'max_iter': 1000.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:10,453] Finished trial#1 with value: 0.49107142857142855 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.04284687918454398, 'hidden_layer_sizes': 940.0, 'max_iter': 900.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:12,846] Finished trial#2 with value: 0.41517857142857145 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.021840254997223106, 'hidden_layer_sizes': 840.0, 'max_iter': 1100.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:13,536] Finished trial#3 with value: 0.34375 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.019700199504777516, 'hidden_layer_sizes': 80.0, 'max_iter': 1500.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:14,940] Finished trial#4 with value: 0.48325892857142855 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.001802036234999756, 'hidden_layer_sizes': 380.0, 'max_iter': 1200.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:54,947] Finished trial#5 with value: 0.8058035714285714 with parameters: {'solver': 'adam', 'learning_rate_init': 0.007889382566248528, 'hidden_layer_sizes': 240.0, 'max_iter': 1300.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:55,949] Finished trial#6 with value: 0.37276785714285715 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.00352586352593507, 'hidden_layer_sizes': 220.0, 'max_iter': 600.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:19:58,388] Finished trial#7 with value: 0.4921875 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.0012988186944369404, 'hidden_layer_sizes': 800.0, 'max_iter': 800.0}. Best is trial#0 with value: 0.8091517857142857.\n",
      "[I 2020-05-24 15:20:38,973] Finished trial#8 with value: 0.8214285714285714 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0015311594161413365, 'hidden_layer_sizes': 110.0, 'max_iter': 1000.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:22:22,682] Finished trial#9 with value: 0.6975446428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.04547607726818422, 'hidden_layer_sizes': 680.0, 'max_iter': 600.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:22:34,416] Finished trial#10 with value: 0.7566964285714286 with parameters: {'solver': 'adam', 'learning_rate_init': 0.004168766845428708, 'hidden_layer_sizes': 50.0, 'max_iter': 200.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:26:06,936] Finished trial#11 with value: 0.7767857142857143 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0010229442332141444, 'hidden_layer_sizes': 550.0, 'max_iter': 2000.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:30:54,120] Finished trial#12 with value: 0.8169642857142857 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0022516999027298763, 'hidden_layer_sizes': 520.0, 'max_iter': 1600.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:33:19,711] Finished trial#13 with value: 0.8180803571428571 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0027581611211981646, 'hidden_layer_sizes': 350.0, 'max_iter': 1800.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:34:02,313] Finished trial#14 with value: 0.8125 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0057395525406201105, 'hidden_layer_sizes': 240.0, 'max_iter': 2000.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:36:24,600] Finished trial#15 with value: 0.8069196428571429 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0025606969870307977, 'hidden_layer_sizes': 380.0, 'max_iter': 1800.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:36:55,923] Finished trial#16 with value: 0.7421875 with parameters: {'solver': 'adam', 'learning_rate_init': 0.008397951620579668, 'hidden_layer_sizes': 140.0, 'max_iter': 1400.0}. Best is trial#8 with value: 0.8214285714285714.\n",
      "[I 2020-05-24 15:39:07,791] Finished trial#17 with value: 0.8236607142857143 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0036990088263871147, 'hidden_layer_sizes': 370.0, 'max_iter': 200.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:40:34,646] Finished trial#18 with value: 0.7979910714285714 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.013565962809101426, 'hidden_layer_sizes': 430.0, 'max_iter': 200.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:41:03,515] Finished trial#19 with value: 0.8169642857142857 with parameters: {'solver': 'adam', 'learning_rate_init': 0.005599011972363877, 'hidden_layer_sizes': 150.0, 'max_iter': 400.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:41:31,804] Finished trial#20 with value: 0.7377232142857143 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.08907386335701434, 'hidden_layer_sizes': 290.0, 'max_iter': 400.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:44:58,591] Finished trial#21 with value: 0.8147321428571429 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0017516405138517433, 'hidden_layer_sizes': 340.0, 'max_iter': 1700.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:48:23,368] Finished trial#22 with value: 0.8147321428571429 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0031115625737708724, 'hidden_layer_sizes': 480.0, 'max_iter': 1800.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:52:21,013] Finished trial#23 with value: 0.8035714285714286 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.004295610857289199, 'hidden_layer_sizes': 630.0, 'max_iter': 700.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:53:32,290] Finished trial#24 with value: 0.8203125 with parameters: {'solver': 'sgd', 'learning_rate_init': 0.0015592240361097366, 'hidden_layer_sizes': 130.0, 'max_iter': 400.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:54:23,484] Finished trial#25 with value: 0.8225446428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0016256325679460165, 'hidden_layer_sizes': 140.0, 'max_iter': 400.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:54:44,746] Finished trial#26 with value: 0.8236607142857143 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0011351017554849128, 'hidden_layer_sizes': 60.0, 'max_iter': 200.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:55:05,145] Finished trial#27 with value: 0.8214285714285714 with parameters: {'solver': 'adam', 'learning_rate_init': 0.00213746614729563, 'hidden_layer_sizes': 50.0, 'max_iter': 200.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:56:17,715] Finished trial#28 with value: 0.8169642857142857 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0010156556723980525, 'hidden_layer_sizes': 220.0, 'max_iter': 300.0}. Best is trial#17 with value: 0.8236607142857143.\n",
      "[I 2020-05-24 15:57:30,879] Finished trial#29 with value: 0.8303571428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0010768790442952027, 'hidden_layer_sizes': 180.0, 'max_iter': 500.0}. Best is trial#29 with value: 0.8303571428571429.\n",
      "[I 2020-05-24 15:58:53,894] Finished trial#30 with value: 0.8225446428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0010894679596428086, 'hidden_layer_sizes': 280.0, 'max_iter': 600.0}. Best is trial#29 with value: 0.8303571428571429.\n",
      "[I 2020-05-24 16:00:31,347] Finished trial#31 with value: 0.8325892857142857 with parameters: {'solver': 'adam', 'learning_rate_init': 0.001093014936728151, 'hidden_layer_sizes': 290.0, 'max_iter': 600.0}. Best is trial#31 with value: 0.8325892857142857.\n",
      "[I 2020-05-24 16:02:37,396] Finished trial#32 with value: 0.8426339285714286 with parameters: {'solver': 'adam', 'learning_rate_init': 0.001152438636114849, 'hidden_layer_sizes': 440.0, 'max_iter': 500.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:05:00,654] Finished trial#33 with value: 0.8236607142857143 with parameters: {'solver': 'adam', 'learning_rate_init': 0.001227082185891, 'hidden_layer_sizes': 450.0, 'max_iter': 800.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:07:23,843] Finished trial#34 with value: 0.8325892857142857 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0012861040545849999, 'hidden_layer_sizes': 450.0, 'max_iter': 900.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:11:14,289] Finished trial#35 with value: 0.8303571428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.001001595619278849, 'hidden_layer_sizes': 600.0, 'max_iter': 1000.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:14:14,481] Finished trial#36 with value: 0.8381696428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.002038608868583399, 'hidden_layer_sizes': 690.0, 'max_iter': 1000.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:16:11,402] Finished trial#37 with value: 0.8203125 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0020415569830694092, 'hidden_layer_sizes': 720.0, 'max_iter': 1200.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:18:18,215] Finished trial#38 with value: 0.8236607142857143 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0013470591855953222, 'hidden_layer_sizes': 800.0, 'max_iter': 900.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:21:00,173] Finished trial#39 with value: 0.8303571428571429 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0019009086437179977, 'hidden_layer_sizes': 520.0, 'max_iter': 1100.0}. Best is trial#32 with value: 0.8426339285714286.\n",
      "[I 2020-05-24 16:21:02,956] Finished trial#40 with value: 0.4575892857142857 with parameters: {'solver': 'lbfgs', 'learning_rate_init': 0.0014241406761355854, 'hidden_layer_sizes': 1000.0, 'max_iter': 800.0}. Best is trial#32 with value: 0.8426339285714286.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "\n",
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    param = {\n",
    "        'objective': 'cross_entropy',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-2, 0.5),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-15, 1),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-15, 4),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 25),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 3, 30),\n",
    "        \n",
    "        'n_estimators': 500,\n",
    "        'num_iterations': 1000,\n",
    "        'max_bin': 256,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': 20,\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train,Y_train)\n",
    "    clf = lgb.train(param, dtrain)\n",
    "    \n",
    "    Ypred_test = clf.predict(X_test)\n",
    "    Ypred_test = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_test)))\n",
    "    \n",
    "    Ypred_train = clf.predict(X_train)\n",
    "    Ypred_train = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_train)))\n",
    "    \n",
    "    score_test = scoreFunction(Ypred_test,Y_test)\n",
    "    score_train = scoreFunction(Ypred_train,Y_train)\n",
    "    \n",
    "    trial.set_user_attr('test_scores', score_test)\n",
    "    trial.set_user_attr('train_scores', score_train)\n",
    "    \n",
    "    return score_test['acc']\n",
    "\n",
    "def objective_nn(trial):\n",
    "\n",
    "    param = {\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'adam', 'sgd']),\n",
    "        'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 1e-3, 0.05),\n",
    "        'hidden_layer_sizes': int(trial.suggest_discrete_uniform('hidden_layer_sizes', 50, 1000, 10)),\n",
    "        'max_iter': int(trial.suggest_discrete_uniform('max_iter', 200, 2000, 100)),\n",
    "        'max_bin': 256,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': 20,\n",
    "    }\n",
    "    \n",
    "    clf = MLPClassifier(random_state=param['random_state'], \n",
    "                        max_iter=param['max_iter'],\n",
    "                        hidden_layer_sizes=(param['hidden_layer_sizes']),\n",
    "                        learning_rate_init=param['learning_rate_init'],\n",
    "                        solver=param['solver'],\n",
    "                       )\n",
    "    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    Ypred_test = clf.predict(X_test)\n",
    "    Ypred_test = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_test)))\n",
    "    \n",
    "    Ypred_train = clf.predict(X_train)\n",
    "    Ypred_train = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_train)))\n",
    "    \n",
    "    score_test = scoreFunction(Ypred_test,Y_test)\n",
    "    score_train = scoreFunction(Ypred_train,Y_train)\n",
    "    \n",
    "    trial.set_user_attr('test_scores', score_test)\n",
    "    trial.set_user_attr('train_scores', score_train)    \n",
    "    \n",
    "    return score_test['acc']\n",
    "    \n",
    "_dataid = 3\n",
    "global_params = params_1[_dataid][0]\n",
    "### get conf of dataid\n",
    "protein_k = global_params[0]\n",
    "rna_k = global_params[1]\n",
    "top_ratio = global_params[2]\n",
    "### read data\n",
    "[data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "[X,Y] = ToMatrix(data,'dense')\n",
    "### split dataset\n",
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,0.2)\n",
    "### dimensionality reduction\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "\n",
    "storage = optuna.storages.redis.RedisStorage(\n",
    "    url='redis://QAZPLMTgv@123@r-bp1ba71b01eb1a94pd.redis.rds.aliyuncs.com:6379/db1',\n",
    ")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_nn, n_trials=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_scores': {'acc': 0.8448660714285714,\n",
       "  'auc': 0.8455456653452016,\n",
       "  'fpr': [0.0, 0.1782608695652174, 1.0],\n",
       "  'tpr': [0.0, 0.8692660550458715, 1.0],\n",
       "  'mcc': 0.6910482567431845,\n",
       "  'tnr': 0.8217391304347826,\n",
       "  'ppv': 0.8221258134490239,\n",
       "  'f_score': 0.8450390189520623,\n",
       "  'ap': 0.7782621340367826,\n",
       "  'brier': 0.15513392857142858,\n",
       "  'sensitivity': 0.8692660550458715},\n",
       " 'train_scores': {'acc': 1.0,\n",
       "  'auc': 1.0,\n",
       "  'fpr': [0.0, 0.0, 1.0],\n",
       "  'tpr': [0.0, 1.0, 1.0],\n",
       "  'mcc': 1.0,\n",
       "  'tnr': 1.0,\n",
       "  'ppv': 1.0,\n",
       "  'f_score': 1.0,\n",
       "  'ap': 1.0,\n",
       "  'brier': 0.0,\n",
       "  'sensitivity': 1.0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# optuna.visualization.plot_intermediate_values(study)\n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "study.best_trial.params\n",
    "study.best_trial.user_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "r = redis.StrictRedis(host='r-bp1ba71b01eb1a94pd.redis.rds.aliyuncs.com', port=6379, db=1,password='QAZPLMTgv@123')\n",
    "r.hmset(getCurrentTime()+'@'+str(_dataid),{\n",
    "    \"dataset\":str(_dataid),\n",
    "    \"params\":json.dumps(study.best_trial.params),\n",
    "    \"train-scores\":json.dumps(study.best_trial.user_attrs['train_scores']),\n",
    "    \"test-scores\":json.dumps(study.best_trial.user_attrs['test_scores']),\n",
    "          })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_cv2 = 5\n",
    "tuning_generalize_ratio2 = 1.0/tuning_cv2 if tuning_cv2!=1 else 0.2\n",
    "\n",
    "df_columns = ['dataid','protein_k','rna_k','top_ratio','training_score','tune_param','scores']\n",
    "df_result2 = pd.DataFrame([],columns=df_columns)\n",
    "\n",
    "for _dataid in [0,1,2,3,4,5]:\n",
    "    INFO('dataid %d'%_dataid)\n",
    "    for global_params in params_1[_dataid]:\n",
    "        start_time = time.time()\n",
    "        ### get conf of dataid\n",
    "        protein_k = global_params[0]\n",
    "        rna_k = global_params[1]\n",
    "        top_ratio = global_params[2]\n",
    "        ### read data\n",
    "        [data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "\n",
    "        for _cv in range(tuning_cv2):\n",
    "            INFO('tuning cv %d'%_cv)\n",
    "            \n",
    "            ### split dataset\n",
    "            [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,tuning_generalize_ratio2)\n",
    "            ### dimensionality reduction\n",
    "            [X_train,X_test,Y_train,Y_test] = \\\n",
    "                        RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "            \n",
    "            for sp in tune_grid[_dataid]:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp)\n",
    "                tune_score = scoreFunction(tune_results[0],tune_results[1])\n",
    "                r = pd.Series({\n",
    "                                'dataid':_dataid,\n",
    "                                'protein_k':protein_k,\n",
    "                                'rna_k':rna_k,\n",
    "                                'top_ratio':top_ratio,\n",
    "                                'training_score':tune_results[2],\n",
    "                                'tune_param':json.dumps(sp),\n",
    "                                'scores':json.dumps(tune_score),\n",
    "                })\n",
    "                df_result2 = df_result2.append(r,ignore_index=True)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "df_result2.to_csv(fname_result2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_from_stage2_result(fpath,group=['dataid']):\n",
    "    df = pd.read_csv(fpath)\n",
    "    acc = [json.loads(row['scores'])['acc'] for idx,row in df.iterrows()]\n",
    "    auc = [json.loads(row['scores'])['auc'] for idx,row in df.iterrows()]\n",
    "    fpr = [json.loads(row['scores'])['fpr'] for idx,row in df.iterrows()]\n",
    "    tpr = [json.loads(row['scores'])['tpr'] for idx,row in df.iterrows()]\n",
    "    mcc = [json.loads(row['scores'])['mcc'] for idx,row in df.iterrows()]\n",
    "    tnr = [json.loads(row['scores'])['tnr'] for idx,row in df.iterrows()]\n",
    "    ppv = [json.loads(row['scores'])['ppv'] for idx,row in df.iterrows()]\n",
    "    f_score = [json.loads(row['scores'])['f_score'] for idx,row in df.iterrows()]\n",
    "    ap = [json.loads(row['scores'])['ap'] for idx,row in df.iterrows()]\n",
    "    brier = [json.loads(row['scores'])['brier'] for idx,row in df.iterrows()]\n",
    "    sensitivity = [json.loads(row['scores'])['sensitivity'] for idx,row in df.iterrows()]\n",
    "#     print(fpr,tpr)\n",
    "    df['acc'] = acc\n",
    "    df['auc'] = auc\n",
    "    df['fpr'] = np.array(fpr)[:,1]\n",
    "    df['tpr'] = np.array(tpr)[:,1]\n",
    "    df['mcc'] = mcc\n",
    "    df['tnr'] = tnr\n",
    "    df['ppv'] = ppv\n",
    "    df['f_score'] = f_score\n",
    "    df['ap'] = ap\n",
    "    df['brier'] = brier\n",
    "    df['sensitivity'] = sensitivity\n",
    "\n",
    "    a = df.groupby(by=group).agg({\n",
    "        'acc':np.mean,\n",
    "        'auc':np.mean,\n",
    "        'fpr':np.mean,\n",
    "        'tpr':np.mean,\n",
    "        'mcc':np.mean,\n",
    "        'tnr':np.mean,\n",
    "        'ppv':np.mean,\n",
    "        'f_score':np.mean,\n",
    "        'ap':np.mean,\n",
    "        'brier':np.mean,\n",
    "        'sensitivity':np.mean,\n",
    "    }).reset_index()\n",
    "#     b = a.join(df.set_index(['dataid','acc']),on=['dataid','acc'],how='inner',lsuffix='_left', rsuffix='_right')\n",
    "    return a\n",
    "\n",
    "# df_tune1 = get_df_from_stage2_result('./result2020-05-09-16-22-10-stage2.csv')\n",
    "# df_sub2_tune1 = get_df_from_stage2_result('./result2020-05-09-21-08-01-stage2.csv')\n",
    "# df_raw = get_df_from_stage2_result('./result2020-05-10-10-30-30-stage2.csv')\n",
    "# df_tune = get_df_from_stage2_result('./result2020-05-09-21-42-23-stage2.csv')\n",
    "\n",
    "df = get_df_from_stage2_result('./result/2020-05-13-21-28-20-stage2-3.csv',\n",
    "                               group=['dataid','protein_k','rna_k','top_ratio','tune_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[ (df['acc']==df['acc'].max()) | (df['auc']==df['auc'].max()) ].values\n",
    "# df['acc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['RNA_K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub2_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_optimal_stage1 = './result/2020-05-04-20-55-26-stage1.csv'\n",
    "df = pd.read_csv(fname_optimal_stage1)\n",
    "acc = [ float(re.findall('[0-9]+\\.[0-9]+',x)[0]) for x in df['test_score'] ]\n",
    "auc = [ float(re.findall('[0-9]+\\.[0-9]+',x)[1]) for x in df['test_score'] ]\n",
    "df['acc'] = acc\n",
    "df['auc'] = auc\n",
    "\n",
    "new_df1 = df.groupby(by=['DATAID','PROTEIN_K','RNA_K']).agg({'acc':np.max,'auc':np.max})\n",
    "new_df2 = df.groupby(by=['DATAID','PROTEIN_K','RNA_K']).agg({'acc':np.mean,'auc':np.mean})\n",
    "inspect_df = df.groupby(by=['DATAID']).agg({'acc':np.max,'auc':np.max})\n",
    "inspect_df.join(df.set_index(['DATAID','acc']),on=['DATAID','acc'],how='inner',lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [json.loads(row['scores'])['acc'] for idx,row in df_result2.iterrows()]\n",
    "auc = [json.loads(row['scores'])['auc'] for idx,row in df_result2.iterrows()]\n",
    "\n",
    "df_result2['acc'] = acc\n",
    "df_result2['auc'] = auc\n",
    "\n",
    "df_result2.groupby(by=['dataid']).agg({'acc':np.mean})\n",
    "# df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "DATAID = 3\n",
    "PROTEIN_K = params_1[DATAID][0][0]\n",
    "RNA_K = params_1[DATAID][0][1]\n",
    "topRatio = params_1[DATAID][0][2]\n",
    "[data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "[X,Y] = ToMatrix(data,'dense')\n",
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=0.96)\n",
    "r1 = LGBTuning(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreFunction(r1[0],r1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "#     ('cb', Tr.DecisionTreeClassifier()),\n",
    "#     ('lgb'+str(i), lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=i)) for i in range(5,20)\n",
    "#     ('lgb2', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=6)),\n",
    "#     ('lgb3', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=3)),\n",
    "#     ('lgb4', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=12)),\n",
    "    ('xgb',xgb.XGBClassifier()),\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegressionCV(cv=10)\n",
    ")\n",
    "\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "scoreFunction(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study_name = 'example-study'  # Unique identifier of the study.\n",
    "study = optuna.create_study(study_name=study_name, \n",
    "                            storage='sqlite:///example.db',\n",
    "                            load_if_exists=True,\n",
    "                            pruner=optuna.pruners.MedianPruner()\n",
    "                           )\n",
    "\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))\n",
    "\n",
    "# study.best_params  # Get best parameters for the objective function.\n",
    "# study.best_value  # Get best objective value.\n",
    "# study.best_trial  # Get best trial's information.\n",
    "# study.trials  # Get all trials' information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
