{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from module.prepare import *\n",
    "from itertools import product\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "# import pygraphviz\n",
    "\n",
    "def scoreFunction(cv_results):\n",
    "    scores = {\n",
    "        'acc':[],\n",
    "        'auc':[],\n",
    "    }\n",
    "\n",
    "    for result in cv_results:\n",
    "        acc = metrics.accuracy_score(result[0],result[1])\n",
    "        auc = metrics.roc_auc_score(result[0],result[1])\n",
    "        scores['acc'].append(acc)\n",
    "        scores['auc'].append(auc)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBTuning(Xtrain,Xtest,Ytrain,Ytest,tuning=True):\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "                             silent=False,\n",
    "                             verbose=1,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=-1,\n",
    "#                              class_weight\n",
    "                            )\n",
    "    \n",
    "    gridParams = {\n",
    "        # step 1\n",
    "#     'learning_rate': [0.01,0.05,0.1],\n",
    "#     'boosting_type':['gbdt','goss'],\n",
    "#     'n_estimators': [50,200,500],\n",
    "#     'num_iterations':[200,400,1000],\n",
    "        # step 1 fixed\n",
    "    'learning_rate': [0.1], ### 0.1\n",
    "    'boosting_type':['gbdt'], ### goss>gbdt\n",
    "    'n_estimators': [500],\n",
    "    'num_iterations':[2000], ### 2000\n",
    "#     'max_depth':[6], ### <6\n",
    "        # step 2\n",
    "    'num_leaves': [200], ### <400<675\n",
    "#     'min_data_in_leaf':[18,20,22], ### 20 default\n",
    "#     'max_bin':[127,255,511],\n",
    "        # step 2 fixed\n",
    "#     'num_leaves': [800],\n",
    "    'max_bin':[256],\n",
    "        # step 3\n",
    "#     'max_depth':[7,8,9,10], ### missed\n",
    "    'colsample_bytree' : [0.75], ### 0.75\n",
    "    'subsample_freq':[1], ### 1\n",
    "#     'subsample' : [0.8], ### 1\n",
    "#     'early_stopping_round':[2],\n",
    "#     'reg_alpha' : [1e-3], ### 0\n",
    "#     'reg_lambda' : [0,0.1,0.5], ### 0\n",
    "    }\n",
    "\n",
    "    if tuning:\n",
    "        grid = GridSearchCV(clf, gridParams,\n",
    "                        scoring='roc_auc',\n",
    "                        verbose=3,\n",
    "                        cv=5,\n",
    "                        n_jobs=1)\n",
    "        print('default params\\n',clf.get_params())\n",
    "        grid.fit(Xtrain,Ytrain)\n",
    "        return grid\n",
    "    else:\n",
    "        arg_str = ''\n",
    "        for k,v in gridParams.items():\n",
    "            if type(v[0])==str:\n",
    "                arg_str += k+'='+\"'\"+v[0]+\"',\"\n",
    "            else:\n",
    "                arg_str += k+'='+str(v[0])+\",\"\n",
    "        eval(\n",
    "            'clf.'+clf.set_params.__name__+\"(\"\n",
    "                +arg_str.rstrip(',')+\n",
    "                \")\"\n",
    "            )\n",
    "#         clf.class_weight = {1:sum(Ytrain==1),0:sum(Ytrain==0)}\n",
    "        print('default params\\n',clf.get_params())\n",
    "        clf.fit(Xtrain,Ytrain)\n",
    "        Ypred = clf.predict(Xtest)\n",
    "        score_train = clf.score(Xtrain,Ytrain)\n",
    "        print('train score %f'%score_train)\n",
    "        return [Ypred,Ytest,score_train,clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def LGBFit(X_train,X_test,Y_train,Y_test):\n",
    "    lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "    lgb_test = lgb.Dataset(X_test, Y_test, reference=lgb_train)\n",
    "\n",
    "    params = {    \n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'roc-auc',\n",
    "#                 'nthread':6,\n",
    "                'learning_rate':0.08,\n",
    "                'num_leaves':300, \n",
    "                'max_depth': -1,   \n",
    "                'subsample': 0.9, \n",
    "                'colsample_bytree': 0.9, \n",
    "#                 'feature_fraction': 1,\n",
    "#                 'bagging_freq': 8,\n",
    "# #                 'num_iterations':300,\n",
    "#                 'min_data_in_leaf':2,\n",
    "#                 'bagging_fraction': 0.8,\n",
    "#                 'num_boost_round':3000,\n",
    "            }\n",
    "\n",
    "    cv_results = lgb.cv(params, lgb_train, nfold=5  \n",
    "                        ,stratified=False, shuffle=True\n",
    "                        ,seed=seed,\n",
    "                        metrics=['auc','binary_logloss','mae']\n",
    "                        ,verbose_eval=1)\n",
    "    print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "    for k,v in cv_results.items():\n",
    "        print('best cv score:', k, pd.Series(cv_results[k]).max())\n",
    "    return [lgb,cv_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    topK =3000\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_params = GetConfigure()\n",
    "num_hyper_params = len(hyper_params)\n",
    "lassocv_param = {'threshold':0.05}\n",
    "\n",
    "cv = 5\n",
    "generalize_ratio = 1.0/cv\n",
    "test_ratio = 1.0/cv\n",
    "mi_use = False\n",
    "\n",
    "tuning_mode = False\n",
    "if tuning_mode:\n",
    "    cv = 1\n",
    "\n",
    "cv_results = []\n",
    "'''\n",
    "    topK =3000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============RETRIEVE TRIAN DATA=================\n",
      "# DEBUG: # DEBUG: **************new dl 0***************\n",
      "# DEBUG: READ SEQ FROM FILE\n",
      "# DEBUG: READ CLUSTER FROM FILE\n",
      "regex error \n",
      "regex error \n",
      "# DEBUG: READ PAIR FROM FILE\n",
      "read line error \n",
      "# DEBUG: GENERATE NEGATIVE PAIR\n",
      "# DEBUG: negative pair number 10412\n",
      "INFO::count of negative pairs10412\n",
      "# DEBUG: PAIR UNION\n",
      "# DEBUG: EXTRACT FEATURES--PROTEIN\n",
      "# DEBUG: EXTRACT FEATURES--RNA\n",
      "# DEBUG: K-MER CALCULATION\n",
      "# DEBUG: FEATURE UNION\n",
      "# DEBUG: GARBAGE COLLECTION\n",
      "MATRIX TRANSFORMATION\n",
      "INFO::cross validation batch 0\n",
      "data shape 20824 10323\n",
      "rf raw data fit score 1.000000\n",
      "select top K features importances 0.9193680987205637\n",
      "dimension remained 3000\n",
      "default params\n",
      " {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.75, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 200, 'objective': 'cross_entropy', 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': False, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 1, 'verbose': 1, 'num_iterations': 2000, 'max_bin': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::cross validation batch 1\n",
      "data shape 20824 10323\n",
      "rf raw data fit score 1.000000\n",
      "select top K features importances 0.9193775242475716\n",
      "dimension remained 3000\n",
      "default params\n",
      " {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.75, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 200, 'objective': 'cross_entropy', 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': False, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 1, 'verbose': 1, 'num_iterations': 2000, 'max_bin': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::cross validation batch 2\n",
      "data shape 20824 10323\n"
     ]
    }
   ],
   "source": [
    "[data,T] = ReadData()\n",
    "\n",
    "# for i in \n",
    "for batch in range(cv):\n",
    "    INFO('cross validation batch %d'%batch)\n",
    "    if mi_use==True:\n",
    "        arr = ToMatrix(data,'sparse')\n",
    "        [X_train,X_test,Y_train,Y_test] = MutualInformationFeatureSelection2(arr,data,generalize_ratio)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    else:\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "        [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "#         X_train,X_test,Y_train,Y_test = IsomapDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = LassoCVFeatureSelection(X_train,X_test,Y_train,Y_test,lassocv_param)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = \\\n",
    "#             PCADimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    if tuning_mode:\n",
    "        [Xtrain,Ytrain] = merge_train_test(X_train,X_test,Y_train,Y_test)\n",
    "        grid = LGBTuning(Xtrain,[],Ytrain,[],True)\n",
    "        cv_results.append(grid)\n",
    "    else:\n",
    "        tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,False)\n",
    "        cv_results.append(tune_results)\n",
    "        scores = scoreFunction(cv_results)\n",
    "#     break\n",
    "\n",
    "scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# DecisionTreePrefit(X_train,X_test,Y_train,Y_test)\n",
    "# [lgb,cv_results] = LGBFit(X_train,X_test,Y_train,Y_test)\n",
    "# WriteResult(DATASET,cv_results,conf,commons)\n",
    "np.mean(scores['acc'][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(grid.cv_results_['mean_test_score'],rug=True,bins=20)\n",
    "plt.show()\n",
    "param_rank = np.array([grid.cv_results_['mean_test_score'],grid.cv_results_['params']]).T\n",
    "a = sorted(param_rank,key=lambda x:x[0],reverse=True)\n",
    "a = np.array(list(a))\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results\n",
    "pca = PCA()\n",
    "pca.explained_variance_ratio_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
