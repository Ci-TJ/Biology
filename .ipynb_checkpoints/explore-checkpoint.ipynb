{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.prepare import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBTuning(X_train,X_test,Y_train,Y_test):\n",
    "    \n",
    "    params = {'boosting_type': 'dart',\n",
    "              'max_depth' : -1,\n",
    "              'objective': 'binary',\n",
    "              'n_jobs': 3, # Updated from nthread\n",
    "              'num_leaves': 400,\n",
    "              'learning_rate': 0.1,\n",
    "              'max_bin': 512,\n",
    "              'subsample_for_bin': 200,\n",
    "              'subsample': 0.8,\n",
    "              'subsample_freq': 1,\n",
    "              'colsample_bytree': 0.8,\n",
    "              'reg_alpha': 0,\n",
    "              'reg_lambda': 0,\n",
    "              'min_split_gain': 0.5,\n",
    "              'min_child_weight': 1,\n",
    "              'min_child_samples': 5,\n",
    "              'scale_pos_weight': 1,\n",
    "              'num_boost_round':3000,\n",
    "              'num_iterations':1000,\n",
    "              'metric' : 'binary_error'}\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(boosting_type=params['boosting_type'],\n",
    "                             max_depth=params['max_depth'],\n",
    "                             objective=params['objective'],\n",
    "                             n_jobs=params['n_jobs'],\n",
    "                             num_leaves=params['num_leaves'],\n",
    "                             learning_rate=params['learning_rate'],\n",
    "                             max_bin=params['max_bin'],\n",
    "                             subsample_for_bin=params['subsample_for_bin'],\n",
    "                             subsample=params['subsample'],\n",
    "                             subsample_freq=params['subsample_freq'],\n",
    "                             colsample_bytree=params['colsample_bytree'],\n",
    "                             reg_alpha=params['reg_alpha'],\n",
    "                             reg_lambda=params['reg_lambda'],\n",
    "                             min_split_gain=params['min_split_gain'],\n",
    "                             min_child_weight=params['min_child_weight'],\n",
    "                             min_child_samples=params['min_child_samples'],\n",
    "                             scale_pos_weight=params['scale_pos_weight'],\n",
    "                             metric=params['metric'],\n",
    "                             num_boost_round=params['num_boost_round'],\n",
    "                             num_iterations=params['num_iterations'],\n",
    "                             silent=False,\n",
    "                             verbose=1\n",
    "                            )\n",
    "    \n",
    "    gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [40],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [seed], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "    print('default params\\n',clf.get_params())\n",
    "\n",
    "    grid = GridSearchCV(clf, gridParams,\n",
    "                    verbose=3,\n",
    "                    cv=5,\n",
    "                    n_jobs=1)\n",
    "\n",
    "    grid.fit(X_train, Y_train)\n",
    "\n",
    "    # Print the best parameters found\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBTuningSingle(X_train,X_test,Y_train,Y_test):\n",
    "    \n",
    "    [X_train, X_test, Y_train, Y_test] = train_test_split(X_train,Y_train,test_size=0.3)\n",
    "    traindata = lgb.Dataset(X_train,Y_train)\n",
    "    testdata = lgb.Dataset(X_test,Y_test)\n",
    "    \n",
    "    params = {'boosting_type': 'gbdt',\n",
    "              'max_depth' : -1,\n",
    "              'objective': 'binary',\n",
    "              'n_jobs': 3, # Updated from nthread\n",
    "              'num_leaves': 400,\n",
    "              'learning_rate': 0.1,\n",
    "              'max_bin': 512,\n",
    "              'subsample_for_bin': 200,\n",
    "              'subsample': 0.8,\n",
    "              'subsample_freq': 1,\n",
    "              'colsample_bytree': 0.8,\n",
    "              'reg_alpha': 0,\n",
    "              'reg_lambda': 0,\n",
    "              'min_split_gain': 0.5,\n",
    "              'min_child_weight': 1,\n",
    "              'min_child_samples': 5,\n",
    "              'scale_pos_weight': 1,\n",
    "              'num_boost_round':3000,\n",
    "              'num_iterations':1000,\n",
    "              'n_estimators':500,\n",
    "              'metric' : 'binary_error'}\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(boosting_type=params['boosting_type'],\n",
    "                             max_depth=params['max_depth'],\n",
    "                             objective=params['objective'],\n",
    "                             n_jobs=params['n_jobs'],\n",
    "                             num_leaves=params['num_leaves'],\n",
    "                             learning_rate=params['learning_rate'],\n",
    "                             max_bin=params['max_bin'],\n",
    "                             subsample_for_bin=params['subsample_for_bin'],\n",
    "                             subsample=params['subsample'],\n",
    "                             subsample_freq=params['subsample_freq'],\n",
    "                             colsample_bytree=params['colsample_bytree'],\n",
    "                             reg_alpha=params['reg_alpha'],\n",
    "                             reg_lambda=params['reg_lambda'],\n",
    "                             min_split_gain=params['min_split_gain'],\n",
    "                             min_child_weight=params['min_child_weight'],\n",
    "                             min_child_samples=params['min_child_samples'],\n",
    "                             scale_pos_weight=params['scale_pos_weight'],\n",
    "                             metric=params['metric'],\n",
    "                             num_boost_round=params['num_boost_round'],\n",
    "                             num_iterations=params['num_iterations'],\n",
    "                             n_estimators=params['n_estimators'],\n",
    "                             silent=False,\n",
    "                             verbose=4,\n",
    "#                              eval_set=(X_test,Y_test)\n",
    "                            )\n",
    "    \n",
    "    gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [40],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [seed], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "    print('default params\\n',clf.get_params())\n",
    "\n",
    "    clf.fit(X_train,Y_train,verbose=4)\n",
    "    print('train',clf.score(X_train,Y_train))\n",
    "    print('test',clf.score(X_test,Y_test))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBFit(X_train,X_test,Y_train,Y_test):\n",
    "    lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "    lgb_test = lgb.Dataset(X_test, Y_test, reference=lgb_train)\n",
    "\n",
    "    params = {    \n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'acc',\n",
    "                'nthread':6,\n",
    "#                 'learning_rate':0.08,\n",
    "                'num_leaves':300, \n",
    "                'max_depth': -1,   \n",
    "                'subsample': 0.9, \n",
    "                'colsample_bytree': 0.9, \n",
    "                'feature_fraction': 1,\n",
    "                'bagging_freq': 8,\n",
    "#                 'num_iterations':300,\n",
    "                'min_data_in_leaf':2,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'num_boost_round':3000,\n",
    "        \n",
    "            }\n",
    "\n",
    "    cv_results = lgb.cv(params, lgb_train, nfold=5  \n",
    "                        ,stratified=False, shuffle=True\n",
    "                        ,seed=seed,\n",
    "                        metrics=['auc','binary_logloss','mae']\n",
    "                        ,verbose_eval=1)\n",
    "    print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "    for k,v in cv_results.items():\n",
    "        print('best cv score:', k, pd.Series(cv_results[k]).max())\n",
    "    return [lgb,cv_results]\n",
    "\n",
    "# lgbclf = lgb.LGBMClassifier(learning_rate=0.045,\n",
    "#                            max_depth=-1,\n",
    "#                            objective='binary',\n",
    "#                             num_leaves=1000,\n",
    "#                             min_child_samples=10,\n",
    "#                             n_estimators=1000,\n",
    "#                             subsample=0.9,\n",
    "#                             random_state=42\n",
    "#                            )\n",
    "# lgbclf.fit(X_train,Y_train)\n",
    "# print('train',lgbclf.score(X_train,Y_train))\n",
    "# print('test',lgbclf.score(X_test,Y_test))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# Y_pred = lgb.predict(X_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[data,T] = ReadData()\n",
    "[X,Y] = ToMatrix(data,'dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y)\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# DecisionTreePrefit(X_train,X_test,Y_train,Y_test)\n",
    "[lgb,cv_results] = LGBFit(X_train,X_test,Y_train,Y_test)\n",
    "WriteResult(DATASET,cv_results,conf,commons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default params\n",
      " {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': 3, 'num_leaves': 400, 'objective': 'binary', 'random_state': None, 'reg_alpha': 0, 'reg_lambda': 0, 'silent': False, 'subsample': 0.8, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'metric': 'binary_error', 'num_boost_round': 3000, 'num_iterations': 1000, 'verbose': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IDE_Project_Programming\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "D:\\IDE_Project_Programming\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9946487376509331\n",
      "test 0.9481434058898848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IDE_Project_Programming\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "clf = LGBTuningSingle(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
