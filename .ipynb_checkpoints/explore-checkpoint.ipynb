{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.prepare import *\n",
    "from itertools import product\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import redis\n",
    "\n",
    "def scoreFunction(result):\n",
    "    scores = {}\n",
    "\n",
    "#     for result in cv_results:\n",
    "    acc = metrics.accuracy_score(result[0],result[1])\n",
    "    auc = metrics.roc_auc_score(result[0],result[1])\n",
    "    scores['acc'] = acc\n",
    "    scores['auc'] = auc\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBTuning(Xtrain,Xtest,Ytrain,Ytest,new_params=None,default=False,replace=False,tuning=True):\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "                             silent=False,\n",
    "                             verbose=1,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=12,\n",
    "#                              class_weight\n",
    "                            )\n",
    "    \n",
    "    gridParams = {\n",
    "        # step 1\n",
    "#     'learning_rate': [0.01,0.05,0.1],\n",
    "#     'boosting_type':['gbdt','goss'],\n",
    "#     'n_estimators': [50,200,500],\n",
    "#     'num_iterations':[200,400,1000],\n",
    "        # step 1 fixed\n",
    "    'learning_rate': [0.1], ### 0.1\n",
    "    'boosting_type':['gbdt'], ### goss>gbdt\n",
    "    'n_estimators': [500],\n",
    "    'num_iterations':[2000], ### 2000\n",
    "#     'max_depth':[6], ### <6\n",
    "        # step 2\n",
    "#     'num_leaves': [200], ### <400<675\n",
    "#     'min_data_in_leaf':[18,20,22], ### 20 default\n",
    "#     'max_bin':[127,255,511],\n",
    "        # step 2 fixed\n",
    "#     'num_leaves': [800],\n",
    "    'max_bin':[256],\n",
    "        # step 3\n",
    "#     'max_depth':[7,8,9,10], ### missed\n",
    "    'colsample_bytree' : [0.9], ### 0.75\n",
    "    'subsample_freq':[1], ### 1\n",
    "        \n",
    "#     'subsample' : [0.7], ### 1\n",
    "#     'min_data_in_leaf':[26],\n",
    "#     'early_stopping_round':[2],\n",
    "#     'reg_alpha' : [1e-3], ### 0\n",
    "#     'reg_lambda' : [0,0.1,0.5], ### 0\n",
    "    }\n",
    "    \n",
    "    if replace is True:\n",
    "        gridParams = {}\n",
    "    if new_params is not None:\n",
    "        gridParams.update(new_params)\n",
    "    \n",
    "\n",
    "    if tuning:\n",
    "        grid = GridSearchCV(clf, gridParams,\n",
    "                        scoring='accuracy',\n",
    "                        verbose=3,\n",
    "                        cv=5,\n",
    "                        n_jobs=1)\n",
    "        print('default params\\n',clf.get_params())\n",
    "        grid.fit(Xtrain,Ytrain)\n",
    "        return grid\n",
    "    else:\n",
    "        if not default:\n",
    "            arg_str = ''\n",
    "            for k,v in gridParams.items():\n",
    "                if type(v[0])==str:\n",
    "                    arg_str += k+'='+\"'\"+v[0]+\"',\"\n",
    "                else:\n",
    "                    arg_str += k+'='+str(v[0])+\",\"\n",
    "            eval(\n",
    "                'clf.'+clf.set_params.__name__+\"(\"\n",
    "                    +arg_str.rstrip(',')+\n",
    "                    \")\"\n",
    "                )\n",
    "#         clf.class_weight = {1:sum(Ytrain==1),0:sum(Ytrain==0)}\n",
    "        print('params\\n',clf.get_params())\n",
    "        clf.fit(Xtrain,Ytrain)\n",
    "        Ypred = clf.predict(Xtest)\n",
    "        score_train = clf.score(Xtrain,Ytrain)\n",
    "        print('train score %f'%score_train)\n",
    "        return [Ypred,Ytest,score_train,clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def LGBFit(X_train,X_test,Y_train,Y_test):\n",
    "    lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "    lgb_test = lgb.Dataset(X_test, Y_test, reference=lgb_train)\n",
    "\n",
    "    params = {    \n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'roc-auc',\n",
    "#                 'nthread':6,\n",
    "                'learning_rate':0.08,\n",
    "                'num_leaves':300, \n",
    "                'max_depth': -1,   \n",
    "                'subsample': 0.9, \n",
    "                'colsample_bytree': 0.9, \n",
    "#                 'feature_fraction': 1,\n",
    "#                 'bagging_freq': 8,\n",
    "# #                 'num_iterations':300,\n",
    "#                 'min_data_in_leaf':2,\n",
    "#                 'bagging_fraction': 0.8,\n",
    "#                 'num_boost_round':3000,\n",
    "            }\n",
    "\n",
    "    cv_results = lgb.cv(params, lgb_train, nfold=5  \n",
    "                        ,stratified=False, shuffle=True\n",
    "                        ,seed=seed,\n",
    "                        metrics=['auc','binary_logloss','mae']\n",
    "                        ,verbose_eval=1)\n",
    "    print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "    for k,v in cv_results.items():\n",
    "        print('best cv score:', k, pd.Series(cv_results[k]).max())\n",
    "    return [lgb,cv_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'data_NPInter10412',\n",
    "'reRPI2825',\n",
    "'RPI488',\n",
    "'RPI2241',\n",
    "'RPITER_RPI1807'\n",
    "'''\n",
    "\n",
    "hyper_params = GetConfigure()\n",
    "num_hyper_params = len(hyper_params)\n",
    "lassocv_param = {'threshold':0.05}\n",
    "\n",
    "cv = 5\n",
    "generalize_ratio = 1.0/cv\n",
    "test_ratio = 1.0/cv\n",
    "\n",
    "mi_use = False\n",
    "outside_grid = True\n",
    "tuning_mode = False\n",
    "\n",
    "if tuning_mode:\n",
    "    cv = 1\n",
    "\n",
    "cv_results = []\n",
    "# scores = []\n",
    "'''\n",
    "    topK =3000\n",
    "'''\n",
    "\n",
    "\n",
    "search_param = [{'learning_rate':[0.05,0.1,0.01,0.02],\n",
    "                'colsample_bytree':[0.7,0.8,0.9,1],\n",
    "                 'max_depth':[5,6,7,8,9],\n",
    "                 'num_iterations':[500,1000,2000],\n",
    "#                  'min_data_in_leaf':[20,25,30]\n",
    "                }]\n",
    "search_grid = list(ParameterGrid(search_param))\n",
    "\n",
    "conf_param = {}\n",
    "\n",
    "\n",
    "cfile = GetConfigureObject()\n",
    "commons = dict(cfile.items('common'))\n",
    "\n",
    "db = redis.StrictRedis(host=commons[str.lower('REDIS_HOST')], port=6379, db=0, \n",
    "                      password=commons[str.lower('REDIS_PWD')], decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[data,T] = ReadData()\n",
    "\n",
    "\n",
    "\n",
    "for batch in range(cv):\n",
    "    INFO('cross validation batch %d'%batch)\n",
    "    if mi_use==True:\n",
    "        arr = ToMatrix(data,'sparse')\n",
    "        [X_train,X_test,Y_train,Y_test] = MutualInformationFeatureSelection2(arr,data,generalize_ratio)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    else:\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "        [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "#         X_train,X_test,Y_train,Y_test = IsomapDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = LassoCVFeatureSelection(X_train,X_test,Y_train,Y_test,lassocv_param)\n",
    "        [X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "#         [X_train,X_test,Y_train,Y_test] = \\\n",
    "#             PCADimensionalityReduction(X_train,X_test,Y_train,Y_test)\n",
    "    if tuning_mode:\n",
    "        [Xtrain,Ytrain] = merge_train_test(X_train,X_test,Y_train,Y_test)\n",
    "        grid = LGBTuning(Xtrain,[],Ytrain,[],True)\n",
    "        cv_results.append(grid)\n",
    "        break\n",
    "    else:\n",
    "        if outside_grid is True:\n",
    "            for sp in search_grid:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp,tuning=False)\n",
    "                r = {'batch':batch,'res':tune_results,'sp':sp,'score':scoreFunction(tune_results)}\n",
    "                cv_results.append(r)\n",
    "            break\n",
    "        else:\n",
    "            tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,tuning=False,default=False)\n",
    "            r = {'batch':batch,'res':tune_results,'score':scoreFunction(tune_results)}\n",
    "            cv_results.append(r)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### compute scores\n",
    "scores = [scoreFunction(r['res'])for r in cv_results]\n",
    "auc = np.mean([x['auc'] for x in scores])\n",
    "acc = np.mean([x['acc'] for x in scores])\n",
    "scores,auc,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### save model\n",
    "if outside_grid is True:\n",
    "    search_param = set( list(map(lambda x:x['sp'],cv_results)) )\n",
    "    scores = [{sp:[] for sp in search_param}]\n",
    "    for sp in search_param:\n",
    "        scores[sp] = np.mean(list( map(lambda x:x['score'],filter(lambda x:x['sp']==sp,cv_results)) ))\n",
    "    best_sp = sorted(scores,key=lambda x:scores[x][0],reverse=True)[0]['sp']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# DecisionTreePrefit(X_train,X_test,Y_train,Y_test)\n",
    "# [lgb,cv_results] = LGBFit(X_train,X_test,Y_train,Y_test)\n",
    "# WriteResult(DATASET,cv_results,conf,commons)\n",
    "np.mean(scores['acc'][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [score for grid in cv_results for score in grid.cv_results_['mean_test_score']]\n",
    "sns.distplot(scores,rug=True,bins=20)\n",
    "plt.show()\n",
    "\n",
    "# param_rank = np.array([grid.cv_results_['mean_test_score'],grid.cv_results_['params']]).T\n",
    "# a = sorted(param_rank,key=lambda x:x[0],reverse=True)\n",
    "# a = np.array(list(a))\n",
    "\n",
    "# a\n",
    "max( cv_results,key=lambda x:np.mean(x.cv_results_['mean_test_score']) ).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list( sorted(grid.best_estimator_.feature_importances_,reverse=True) )\n",
    "sum(feature_importance[:2000])/sum(feature_importance)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(grid.best_estimator_.feature_importances_,bins=100)\n",
    "plt.xlabel('feature importance')\n",
    "plt.ylabel('ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
